{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTTFSZ0bTjtX"
      },
      "source": [
        "# Partition-Based Active Learning for Graph Neural Networks - Demo Notebook\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-nblNuSNKXj"
      },
      "source": [
        "## Library Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZfT64h5BS4P"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aHNUhhO-xW1G"
      },
      "outputs": [],
      "source": [
        "!pip install dgl\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "vNYKbt5RNKXk"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import os\n",
        "import argparse\n",
        "import json\n",
        "import codecs\n",
        "from timeit import default_timer as timer\n",
        "from copy import deepcopy\n",
        "\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from sklearn import cluster\n",
        "import sklearn.metrics as metrics\n",
        "\n",
        "import dgl\n",
        "import networkx as nx\n",
        "from networkx.algorithms.link_analysis.pagerank_alg import pagerank\n",
        "from networkx.algorithms.community.quality import modularity\n",
        "from networkx.utils.mapped_queue import MappedQueue\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.nn import GCNConv, SAGEConv, GATConv\n",
        "from torch_geometric.datasets import Planetoid, CoraFull, Coauthor\n",
        "from ogb.nodeproppred import PygNodePropPredDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Queries"
      ],
      "metadata": {
        "id": "xYiNUGA8vY9n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "TVhsn-EEKEsS"
      },
      "outputs": [],
      "source": [
        "class ActiveLearning:\n",
        "    \"\"\"\n",
        "    An active learning framework that...\n",
        "    * queries from an oracle;\n",
        "    * updates its known set,\n",
        "    * trains the GNN model, and\n",
        "    * evaluate the Macro F-1 score.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        self.round = 0\n",
        "        self.data = data\n",
        "        self.model = model\n",
        "        self.seed = seed\n",
        "        self.args = args\n",
        "        self.retrain = args.retrain\n",
        "        self.clf = None\n",
        "        self.aggregated = None\n",
        "        self.num_centers = args.num_centers\n",
        "        self.num_parts = -1\n",
        "\n",
        "    def query(self, b):\n",
        "        pass\n",
        "\n",
        "    def update(self, train_mask):\n",
        "        self.data.train_mask = train_mask\n",
        "        self.round += 1\n",
        "\n",
        "    def train(self):\n",
        "        if self.retrain:\n",
        "            self.clf = deepcopy(self.model).to(self.args.device)\n",
        "        else:\n",
        "            self.clf = self.model.to(self.args.device)\n",
        "        optimizer = optim.Adam(\n",
        "            self.clf.parameters(), lr=self.args.lr,\n",
        "            weight_decay=self.args.weight_decay)\n",
        "        for epoch in range(self.args.epochs):\n",
        "            self.clf.train()\n",
        "            optimizer.zero_grad()\n",
        "            out = self.clf(self.data.x, self.data.adj_t)\n",
        "            true = self.data.y\n",
        "            if len(true.shape) > 1:\n",
        "                true = true.squeeze(1)\n",
        "            loss = F.cross_entropy(\n",
        "                out[self.data.train_mask],\n",
        "                true[self.data.train_mask])\n",
        "            if self.args.verbose == 2:\n",
        "                print('Epoch {:03d}: Training loss: {:.4f}'.format(epoch, loss))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    def evaluate(self):\n",
        "        self.clf.eval()\n",
        "        logits = self.clf(self.data.x, self.data.adj_t)\n",
        "        y_pred = logits.max(1)[1].cpu()\n",
        "        y_true = self.data.y.cpu()\n",
        "        f1 = metrics.f1_score(y_true, y_pred, average='macro')\n",
        "        acc = metrics.f1_score(y_true, y_pred, average='micro')\n",
        "        if self.args.verbose == 2:\n",
        "            print('Macro-f1 score: {:.4f}'.format(f1))\n",
        "            print('Micro-f1 score: {:.4f}'.format(acc))\n",
        "        return f1, acc\n",
        "\n",
        "    def get_node_representation(self, rep='aggregation', encoder='gcn'):\n",
        "\n",
        "        if rep == 'aggregation':\n",
        "            if self.aggregated is None:\n",
        "\n",
        "                # Dense Implementation\n",
        "\n",
        "                # A_ = self.data.adj_t.to_dense() + torch.eye(self.data.num_nodes, device=self.args.device)  # A + I\n",
        "                # D_ = torch.diag(A_.sum(dim=0) ** (-1 / 2))  # (A + I)^(-1/2)\n",
        "                # A_norm = torch.sparse.mm(torch.sparse.mm(D_, A_), D_)\n",
        "                # self.aggregated = A_norm\n",
        "                # for i in range(self.clf.num_layers - 1):\n",
        "                #     self.aggregated = torch.sparse.mm(self.aggregated, A_norm)\n",
        "                # self.aggregated = torch.sparse.mm(self.aggregated, self.data.x)\n",
        "\n",
        "                # Sparse Implementation\n",
        "\n",
        "                # nnz = len(self.data.adj_t.storage._row)\n",
        "                # indice = torch.cat([self.data.adj_t.storage._row.unsqueeze(dim=0),\n",
        "                #                     self.data.adj_t.storage._col.unsqueeze(dim=0)], dim=0)\n",
        "                # values = torch.ones(nnz, device=self.args.device)\n",
        "                # A = torch.sparse_coo_tensor(indice, values, [self.data.num_nodes, self.data.num_nodes])\n",
        "                # diag = torch.tensor(range(self.data.num_nodes), device=self.args.device).unsqueeze(dim=0)\n",
        "                # indice = torch.cat([diag, diag], dim=0)\n",
        "                # values = torch.ones(self.data.num_nodes, device=self.args.device)\n",
        "                # I = torch.sparse_coo_tensor(indice, values, [self.data.num_nodes, self.data.num_nodes])\n",
        "                # A = A + I\n",
        "                # value = torch.sparse.sum(A, dim=0) ** (-1 / 2)\n",
        "                # D = torch.sparse_coo_tensor(indice, value.to_dense(), [self.data.num_nodes, self.data.num_nodes])\n",
        "                # A_norm = torch.sparse.mm(torch.sparse.mm(D, A), D)\n",
        "                # self.aggregated = self.data.x.to_sparse()\n",
        "                # for i in range(self.clf.num_layers):\n",
        "                #     self.aggregated = torch.sparse.mm(A_norm, self.aggregated)\n",
        "                # self.aggregated = self.aggregated.to_dense()\n",
        "\n",
        "                feat_dim = self.data.x.size(1)\n",
        "                if encoder == 'sage':\n",
        "                    conv = SAGEConv(feat_dim, feat_dim, bias=False)\n",
        "                    conv.lin_l.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                    conv.lin_r.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                else:\n",
        "                    conv = GCNConv(feat_dim, feat_dim, cached=True, bias=False)\n",
        "                    conv.lin.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "                conv.to(self.args.device)\n",
        "                with torch.no_grad():\n",
        "                    self.aggregated = conv(self.data.x, self.data.adj_t)\n",
        "                    self.aggregated = conv(self.aggregated, self.data.adj_t)\n",
        "            return self.aggregated\n",
        "\n",
        "        elif rep == 'embedding':\n",
        "            with torch.no_grad():\n",
        "                embed = self.clf.embed(self.data.x, self.data.adj_t)\n",
        "            return embed\n",
        "\n",
        "        else:\n",
        "            return self.data.x\n",
        "\n",
        "    def split_cluster(self, b, partitions, x_embed=None, method='default'):\n",
        "\n",
        "        if method == 'inertia':\n",
        "            part_size = []\n",
        "            for i in range(self.num_parts):\n",
        "                part_id = np.where(partitions == i)[0]\n",
        "                x = x_embed[part_id]\n",
        "                kmeans = Cluster(n_clusters=1, n_dim=x_embed.shape[1], seed=self.seed, device=self.args.device)\n",
        "                kmeans.train(x.cpu())\n",
        "                inertia = kmeans.get_inertia()\n",
        "                part_size.append(inertia)\n",
        "\n",
        "            part_size = np.rint(b * np.array(part_size) / sum(part_size)).astype(int)\n",
        "            part_size = np.maximum(self.num_centers, part_size)\n",
        "            i = 0\n",
        "            while part_size.sum() - b != 0:\n",
        "                if part_size.sum() - b > 0:\n",
        "                    i = self.num_parts - 1 if i <= 0 else i\n",
        "                    while part_size[i] <= 1:\n",
        "                        i -= 1\n",
        "                    part_size[i] -= 1\n",
        "                    i -= 1\n",
        "                else:\n",
        "                    i = 0 if i >= self.num_parts else i\n",
        "                    part_size[i] += 1\n",
        "                    i += 1\n",
        "\n",
        "        elif method == 'size':\n",
        "            part_size = []\n",
        "            for i in range(self.num_parts):\n",
        "                part_size.append(len(np.where(partitions == i)[0]))\n",
        "            part_size = np.rint(b * np.array(part_size) / sum(part_size)).astype(int)\n",
        "            part_size = np.maximum(self.num_centers, part_size)\n",
        "            i = 0\n",
        "            while part_size.sum() - b != 0:\n",
        "                if part_size.sum() - b > 0:\n",
        "                    i = self.num_parts - 1 if i <= 0 else i\n",
        "                    while part_size[i] <= 1:\n",
        "                        i -= 1\n",
        "                    part_size[i] -= 1\n",
        "                    i -= 1\n",
        "                else:\n",
        "                    i = 0 if i >= self.num_parts else i\n",
        "                    part_size[i] += 1\n",
        "                    i += 1\n",
        "\n",
        "        else:\n",
        "            part_size = [b // self.num_parts for _ in range(self.num_parts)]\n",
        "            for i in range(b % self.num_parts):\n",
        "                part_size[i] += 1\n",
        "\n",
        "        return part_size\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Active Learning Agent (uninitialized)\"\n",
        "\n",
        "\n",
        "class Random(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Random:\n",
        "    The Random Sampling method chooses nodes uniformly at random,\n",
        "    similarly as the commonly used semi-supervised learning experiment setting for GCN.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Random, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        indice = np.random.choice(\n",
        "            np.where(self.data.train_mask == 0)[0], b, replace=False\n",
        "        )\n",
        "        return torch.tensor(indice)\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Random\"\n",
        "\n",
        "\n",
        "class Density(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Density:\n",
        "    The Density method first performs a clustering algorithm on the hidden representations of the nodes,\n",
        "    and then chooses nodes with maximum density score, which is (approximately) inversely proportional to\n",
        "    the L2-distance between each node and its cluster center.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Density, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        # Get propagated nodes\n",
        "        x_embed = self.get_node_representation('embedding').cpu()\n",
        "\n",
        "        # Perform K-Means as approximation\n",
        "        kmeans = Cluster(n_clusters=b, n_dim=x_embed.shape[1], seed=self.seed, device=self.args.device)\n",
        "        kmeans.train(x_embed)\n",
        "\n",
        "        # Calculate density\n",
        "        centers = kmeans.get_centroids()\n",
        "        label = kmeans.predict(x_embed)\n",
        "        centers = centers[label]\n",
        "        dist_map = torch.linalg.norm(x_embed - centers, dim=1)\n",
        "        density = 1 / (1 + dist_map)\n",
        "\n",
        "        density[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(density, k=b)\n",
        "\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Density\"\n",
        "\n",
        "\n",
        "class Uncertainty(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Uncertainty:\n",
        "    The Uncertainty method chooses the nodes with maximum entropy on the predicted class distribution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Uncertainty, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        logits = self.clf(self.data.x, self.data.adj_t)\n",
        "        entropy = -torch.sum(F.softmax(logits, dim=1) * F.log_softmax(logits, dim=1), dim=1)\n",
        "        entropy[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(entropy, k=b)\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Uncertainty\"\n",
        "\n",
        "\n",
        "class CoreSetGreedy(ActiveLearning):\n",
        "    \"\"\"\n",
        "    CoreSet:\n",
        "    The CoreSet method performs a K-Center clustering over the hidden representations of nodes.\n",
        "    A time-efficient greedy approximation version by choosing node closest to the cluster centers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(CoreSetGreedy, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "\n",
        "        embed = self.get_node_representation('embedding').cpu()\n",
        "        indices = list(np.where(self.data.train_mask != 0)[0])\n",
        "\n",
        "        for i in range(b):\n",
        "            dist = metrics.pairwise_distances(embed, embed[indices], metric='euclidean')\n",
        "            min_distances = torch.min(torch.tensor(dist), dim=1)[0]\n",
        "            new_index = min_distances.argmax()\n",
        "            indices.append(int(new_index))\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Core Set (Greedy)\"\n",
        "\n",
        "\n",
        "class CoreSetMIP(ActiveLearning):\n",
        "    \"\"\"\n",
        "    CoreSet:\n",
        "    The CoreSet method performs a K-Center clustering over the hidden representations of nodes.\n",
        "    Optimized by gurobipy MIP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(CoreSetMIP, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        import gurobipy\n",
        "\n",
        "        # Get distance matrix\n",
        "        embed = self.get_node_representation('embedding')\n",
        "        dist_mat = embed.matmul(embed.t())\n",
        "        sq = dist_mat.diagonal().reshape(self.data.num_nodes, 1)\n",
        "        dist_mat = torch.sqrt(-dist_mat * 2 + sq + sq.t())\n",
        "\n",
        "        # Perform greedy K-center\n",
        "        mask = self.data.train_mask.copy()\n",
        "        mat = dist_mat[~mask, :][:, mask]\n",
        "        _, indices = mat.min(dim=1)[0].topk(k=b)\n",
        "        indices = torch.arange(self.data.num_nodes)[~mask][indices]\n",
        "        mask[indices] = True\n",
        "\n",
        "        # Robust approximation\n",
        "        opt = mat.min(dim=1)[0].max()\n",
        "        ub = opt\n",
        "        lb = opt / 2.0\n",
        "        xx, yy = np.where(dist_mat <= opt)\n",
        "        dd = dist_mat[xx, yy]\n",
        "\n",
        "        flag = self.data.train_mask.copy()\n",
        "        subset = np.where(flag == 0)[0].tolist()\n",
        "\n",
        "        # Solve MIP for fac_loc\n",
        "        x = {}\n",
        "        y = {}\n",
        "        z = {}\n",
        "        n = self.data.num_nodes\n",
        "        m = len(xx)\n",
        "\n",
        "        model = gurobipy.Model(\"k-center\")\n",
        "        for i in range(n):\n",
        "            z[i] = model.addVar(\n",
        "                obj=1, ub=0.0, vtype=\"B\", name=\"z_{}\".format(i))\n",
        "\n",
        "        for i in range(m):\n",
        "            _x = xx[i]\n",
        "            _y = yy[i]\n",
        "            if _y not in y:\n",
        "                if _y in subset:\n",
        "                    y[_y] = model.addVar(\n",
        "                        obj=0, ub=1.0, lb=1.0, vtype=\"B\", name=\"y_{}\".format(_y))\n",
        "                else:\n",
        "                    y[_y] = model.addVar(\n",
        "                        obj=0, vtype=\"B\", name=\"y_{}\".format(_y))\n",
        "            x[_x, _y] = model.addVar(\n",
        "                obj=0, vtype=\"B\", name=\"x_{},{}\".format(_x, _y))\n",
        "        model.update()\n",
        "\n",
        "        coef = [1 for j in range(n)]\n",
        "        var = [y[j] for j in range(n)]\n",
        "        model.addConstr(\n",
        "            gurobipy.LinExpr(coef, var), \"=\", rhs=b + len(subset), name=\"k_center\")\n",
        "\n",
        "        for i in range(m):\n",
        "            _x = xx[i]\n",
        "            _y = yy[i]\n",
        "            model.addConstr(\n",
        "                x[_x, _y], \"<\", y[_y], name=\"Strong_{},{}\".format(_x, _y))\n",
        "\n",
        "        yyy = {}\n",
        "        for v in range(m):\n",
        "            _x = xx[v]\n",
        "            _y = yy[v]\n",
        "            if _x not in yyy:\n",
        "                yyy[_x] = []\n",
        "            if _y not in yyy[_x]:\n",
        "                yyy[_x].append(_y)\n",
        "\n",
        "        for _x in yyy:\n",
        "            coef = []\n",
        "            var = []\n",
        "            for _y in yyy[_x]:\n",
        "                coef.append(1)\n",
        "                var.append(x[_x, _y])\n",
        "            coef.append(1)\n",
        "            var.append(z[_x])\n",
        "            model.addConstr(\n",
        "                gurobipy.LinExpr(coef, var), \"=\", 1, name=\"Assign{}\".format(_x))\n",
        "\n",
        "        # Approximate\n",
        "        delta = 1e-7\n",
        "        sol_file = None\n",
        "        while ub - lb > delta:\n",
        "            cur_r = (ub + lb) / 2.0\n",
        "            viol = np.where(dd > cur_r)\n",
        "            new_max_d = torch.min(dd[dd >= cur_r])\n",
        "            new_min_d = torch.max(dd[dd <= cur_r])\n",
        "            for v in viol[0]:\n",
        "                x[xx[v], yy[v]].UB = 0\n",
        "\n",
        "            model.update()\n",
        "            r = model.optimize()\n",
        "            if model.getAttr(gurobipy.GRB.Attr.Status) == gurobipy.GRB.INFEASIBLE:\n",
        "                failed = True\n",
        "                print(\"Infeasible\")\n",
        "            elif sum([z[i].X for i in range(len(z))]) > 0:\n",
        "                failed = True\n",
        "                print(\"Failed\")\n",
        "            else:\n",
        "                failed = False\n",
        "            if failed:\n",
        "                lb = max(cur_r, new_max_d)\n",
        "                for v in viol[0]:\n",
        "                    x[xx[v], yy[v]].UB = 1\n",
        "            else:\n",
        "                print(\"sol founded\", cur_r, lb, ub)\n",
        "                ub = min(cur_r, new_min_d)\n",
        "                sol_file = \"s_{}_solution_{}.sol\".format(b, cur_r)\n",
        "                model.write(sol_file)\n",
        "\n",
        "        # Process results\n",
        "        if sol_file is not None:\n",
        "            results = open(sol_file).read().split('\\n')\n",
        "            results_nodes = filter(lambda x1: 'y' in x1,\n",
        "                                   filter(lambda x1: '#' not in x1, results))\n",
        "            string_to_id = lambda x1: (\n",
        "                int(x1.split(' ')[0].split('_')[1]),\n",
        "                int(x1.split(' ')[1]))\n",
        "            result_node_ids = map(string_to_id, results_nodes)\n",
        "            centers = []\n",
        "            for node_result in result_node_ids:\n",
        "                if node_result[1] > 0:\n",
        "                    centers.append(node_result[0])\n",
        "            return torch.tensor(centers)\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Core Set (MIP)\"\n",
        "\n",
        "\n",
        "class Degree(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Centrality:\n",
        "    The Centrality method chooses nodes with the largest graph centrality metric value.\n",
        "    This framework chooses node degree as the metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(Degree, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "\n",
        "        if hasattr(self.data.adj_t.storage, '_row'):\n",
        "            degree = self.data.adj_t.sum(dim=0)\n",
        "        else:\n",
        "            indice = torch.cat([self.data.adj_t[0].unsqueeze(dim=0),\n",
        "                                self.data.adj_t[1].unsqueeze(dim=0)], dim=0)\n",
        "            values = torch.ones(self.data.adj_t.shape[1], device=self.args.device)\n",
        "            adj = torch.sparse_coo_tensor(indice, values, [self.data.num_nodes, self.data.num_nodes]).to_dense()\n",
        "            degree = adj.sum(dim=0)\n",
        "\n",
        "        degree[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(degree, k=b)\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Centrality (Degree)\"\n",
        "\n",
        "\n",
        "class PageRank(ActiveLearning):\n",
        "    \"\"\"\n",
        "    PageRank:\n",
        "    The Centrality method chooses nodes with the largest graph centrality metric value.\n",
        "    This framework chooses node degree as the metric.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(PageRank, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "        page = torch.tensor(list(pagerank(self.data.g).values()))\n",
        "        page[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(page, k=b)\n",
        "        return indices\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"Centrality (PageRank)\"\n",
        "\n",
        "\n",
        "class AGE(ActiveLearning):\n",
        "    \"\"\"\n",
        "    AGE:\n",
        "    AGE defines the informativeness of nodes by linearly combining three metrics:\n",
        "    centrality, density and uncertainty.\n",
        "    It further chooses nodes with the highest scores.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args):\n",
        "        super(AGE, self).__init__(data, model, seed, args)\n",
        "\n",
        "    def query(self, b):\n",
        "\n",
        "        # Get entropy\n",
        "        logits = self.clf(self.data.x, self.data.adj_t)\n",
        "        entropy = -torch.sum(F.softmax(logits, dim=1) * F.log_softmax(logits, dim=1), dim=1)\n",
        "\n",
        "        # Get centrality\n",
        "        page = torch.tensor(list(pagerank(self.data.g).values()),\n",
        "                            dtype=logits.dtype, device=self.args.device)\n",
        "\n",
        "        # Get density\n",
        "        x = self.get_node_representation('embedding').cpu()\n",
        "        N = x.shape[0]\n",
        "\n",
        "        kmeans = Cluster(n_clusters=b, n_dim=x.shape[1], seed=self.seed, device=self.args.device)\n",
        "        kmeans.train(x)\n",
        "        centers = kmeans.get_centroids()\n",
        "        label = kmeans.predict(x)\n",
        "\n",
        "        x = x.to(logits.device)\n",
        "        centers = torch.tensor(centers[label], dtype=x.dtype, device=x.device)\n",
        "        dist_map = torch.linalg.norm(x - centers, dim=1).to(logits.dtype)\n",
        "        density = 1 / (1 + dist_map)\n",
        "\n",
        "        # Get percentile\n",
        "        percentile = (torch.arange(N, dtype=logits.dtype, device=self.args.device) / N)\n",
        "        id_sorted = density.argsort(descending=False)\n",
        "        density[id_sorted] = percentile\n",
        "        id_sorted = entropy.argsort(descending=False)\n",
        "        entropy[id_sorted] = percentile\n",
        "        id_sorted = page.argsort(descending=False)\n",
        "        page[id_sorted] = percentile\n",
        "\n",
        "        # Get linear combination\n",
        "        alpha, beta, gamma = self.data.params['age']\n",
        "        age_score = alpha * entropy + beta * density + gamma * page\n",
        "        age_score[np.where(self.data.train_mask != 0)[0]] = 0\n",
        "        _, indices = torch.topk(age_score, k=b)\n",
        "        return indices\n",
        "\n",
        "\n",
        "class ClusterBased(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Cluster:\n",
        "    The cluster method first performs clustering (K-Means as approximation of K-Medoids)\n",
        "    on the aggregated node features and then choose the nodes closest to the K-means centers.\n",
        "\n",
        "    rep {'feature', 'embedding', 'aggregation'}\n",
        "    init {‘k-means++’, ‘random’}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args,\n",
        "                 representation='aggregation',\n",
        "                 encoder='gcn',\n",
        "                 initialization='k-means++'):\n",
        "        super(ClusterBased, self).__init__(data, model, seed, args)\n",
        "        self.representation = representation\n",
        "        self.encoder = encoder\n",
        "        self.initialization = None if initialization != 'k-means++' else initialization\n",
        "\n",
        "    def query(self, b):\n",
        "\n",
        "        # Get node representations\n",
        "        x = self.get_node_representation(self.representation, self.encoder)\n",
        "\n",
        "        # Perform K-Means clustering:\n",
        "        kmeans = Cluster(n_clusters=b, n_dim=x.shape[1], seed=self.seed, device=self.args.device)\n",
        "        kmeans.train(x.cpu().numpy())\n",
        "        centers = torch.tensor(kmeans.get_centroids(), dtype=x.dtype, device=x.device)\n",
        "\n",
        "        # Obtain the centers\n",
        "        indices = list(np.where(self.data.train_mask != 0)[0])\n",
        "        for center in centers:\n",
        "            center = center.to(dtype=x.dtype, device=x.device)\n",
        "            dist_map = torch.linalg.norm(x - center, dim=1)\n",
        "            dist_map[indices] = torch.tensor(np.infty, dtype=dist_map.dtype, device=dist_map.device)\n",
        "            idx = int(torch.argmin(dist_map))\n",
        "            indices.append(idx)\n",
        "\n",
        "        return torch.tensor(indices)\n",
        "\n",
        "\n",
        "class PartitionBased(ActiveLearning):\n",
        "    \"\"\"\n",
        "    Partition:\n",
        "    Our method, which first partitions the graph into communities, and\n",
        "    performs clustering over each graph community on the aggregated node features.\n",
        "\n",
        "    rep {'none', 'embed', 'prop'}\n",
        "    init {‘k-means++’, ‘random’}\n",
        "    compensation {float: 0 - 1}\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, model, seed, args,\n",
        "                 representation='aggregation',\n",
        "                 encoder='gcn',\n",
        "                 initialization='k-means++',\n",
        "                 compensation=1):\n",
        "        super(PartitionBased, self).__init__(data, model, seed, args)\n",
        "        self.representation = representation\n",
        "        self.encoder = encoder\n",
        "        self.initialization = None if initialization != 'k-means++' else initialization\n",
        "        self.compensation = compensation\n",
        "\n",
        "    def query(self, b):\n",
        "\n",
        "        # Perform graph partition (preprocessed)\n",
        "        self.num_parts = int(np.ceil(b / self.num_centers))\n",
        "        compensation = 0\n",
        "        if self.num_parts > self.data.max_part:\n",
        "            self.num_parts = self.data.max_part\n",
        "            compensation = self.compensation\n",
        "        partitions = np.array(self.data.partitions[self.num_parts].cpu())\n",
        "\n",
        "        # Get node representations\n",
        "        x = self.get_node_representation(self.representation, self.encoder)\n",
        "\n",
        "        # Determine the number of partitions and number of centers\n",
        "        part_size = self.split_cluster(b, partitions, x)\n",
        "\n",
        "        # Iterate over each partition\n",
        "        indices = list(np.where(self.data.train_mask != 0)[0])\n",
        "        for i in range(self.num_parts):\n",
        "            part_id = np.where(partitions == i)[0]\n",
        "            masked_id = [i for i, x in enumerate(part_id) if x in indices]\n",
        "            xi = x[part_id]\n",
        "\n",
        "            n_clusters = part_size[i]\n",
        "            if n_clusters <= 0:\n",
        "                continue\n",
        "\n",
        "            # Perform K-Means clustering:\n",
        "            kmeans = Cluster(n_clusters=n_clusters, n_dim=xi.shape[1], seed=self.seed, device=self.args.device)\n",
        "            kmeans.train(xi.cpu().numpy())\n",
        "            centers = kmeans.get_centroids()\n",
        "\n",
        "            # Compensating for the interference across partitions\n",
        "            dist = None\n",
        "            if self.compensation > 0:\n",
        "                dist_to_center = torch.ones(x.shape[0], dtype=x.dtype, device=x.device) * np.infty\n",
        "                for idx in indices:\n",
        "                    dist_to_center = torch.minimum(dist_to_center, torch.linalg.norm(x - x[idx], dim=1))\n",
        "                dist = dist_to_center[part_id]\n",
        "\n",
        "            # Obtain the centers\n",
        "            for center in centers:\n",
        "                center = torch.tensor(center, dtype=x.dtype, device=x.device)\n",
        "                dist_map = torch.linalg.norm(xi - center, dim=1)\n",
        "                if self.compensation > 0:\n",
        "                    dist_map -= dist * compensation\n",
        "                dist_map[masked_id] = torch.tensor(np.infty, dtype=dist_map.dtype, device=dist_map.device)\n",
        "                idx = int(torch.argmin(dist_map))\n",
        "                masked_id.append(idx)\n",
        "                indices.append(part_id[idx])\n",
        "\n",
        "        return torch.tensor(indices)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Models"
      ],
      "metadata": {
        "id": "scr7GAPlvaeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Cluster:\n",
        "    \"\"\"\n",
        "    Kmeans Clustering\n",
        "    \"\"\"\n",
        "    def __init__(self, n_clusters, n_dim, seed,\n",
        "                 implementation='sklearn',\n",
        "                 init='k-means++',\n",
        "                 device=torch.cuda.is_available()):\n",
        "\n",
        "        assert implementation in ['sklearn', 'faiss', 'cuml']\n",
        "        assert init in ['k-means++', 'random']\n",
        "\n",
        "        self.n_clusters = n_clusters\n",
        "        self.n_dim = n_dim\n",
        "        self.implementation = implementation\n",
        "        self.initialization = init\n",
        "        self.model = None\n",
        "\n",
        "        if implementation == 'sklearn':\n",
        "            self.model = cluster.KMeans(n_clusters=n_clusters, init=init, random_state=seed)\n",
        "        elif implementation == 'faiss':\n",
        "            import faiss\n",
        "            self.model = faiss.Kmeans(n_dim, n_clusters, niter=20, nredo=10, seed=seed, gpu=device != 'cpu')\n",
        "        elif implementation == 'cuml':\n",
        "            import cuml\n",
        "            if init == 'k-means++':\n",
        "                init = 'scalable-kmeans++'\n",
        "            self.model = cuml.KMeans(n_dim, n_clusters, random_state=seed, init=init, output_type='numpy')\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "    def train(self, x):\n",
        "        if self.implementation == 'sklearn':\n",
        "            self.model.fit(x)\n",
        "        elif self.implementation == 'faiss':\n",
        "            if self.initialization == 'kmeans++':\n",
        "                init_centroids = self._kmeans_plusplus(x, self.n_clusters).cpu().numpy()\n",
        "            else:\n",
        "                init_centroids = None\n",
        "            self.model.train(x, init_centroids=init_centroids)\n",
        "        elif self.implementation == 'cuml':\n",
        "            self.model.fit(x)\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "    def predict(self, x):\n",
        "        if self.implementation == 'sklearn':\n",
        "            return self.model.predict(x)\n",
        "        elif self.implementation == 'faiss':\n",
        "            _, labels = self.model.index.search(x, 1)\n",
        "            return labels\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "    def get_centroids(self):\n",
        "        if self.implementation == 'sklearn':\n",
        "            return self.model.cluster_centers_\n",
        "        elif self.implementation == 'faiss':\n",
        "            return self.model.centroids\n",
        "        elif self.implementation == 'cuml':\n",
        "            return self.model.cluster_centers_\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "    def get_inertia(self):\n",
        "        if self.implementation == 'sklearn':\n",
        "            return self.model.inertia_\n",
        "        else:\n",
        "            raise NotImplemented\n",
        "\n",
        "    @staticmethod\n",
        "    def _kmeans_plusplus(X, n_clusters):\n",
        "        \"\"\"\n",
        "        K-means++ initialization in PyTorch for Faiss.\n",
        "\n",
        "        Modified from sklearn version of implementation.\n",
        "        https://github.com/scikit-learn/scikit-learn/blob/2beed5584/sklearn/cluster/_kmeans.py\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        # Set the number of local seeding trials if none is given\n",
        "        n_local_trials = 2 + int(np.log(n_clusters))\n",
        "\n",
        "        # Pick first center randomly and track index of point\n",
        "        center_id = torch.randint(n_samples, (1,)).item()\n",
        "        centers = [X[center_id]]\n",
        "\n",
        "        # Initialize list of closest distances and calculate current potential\n",
        "        closest_dist_sq = torch.cdist(X, X[center_id].unsqueeze(dim=0)).pow(2).squeeze()\n",
        "        current_pot = closest_dist_sq.sum()\n",
        "\n",
        "        # Pick the remaining n_clusters-1 points\n",
        "        for c in range(1, n_clusters):\n",
        "            # Choose center candidates by sampling with probability proportional\n",
        "            # to the squared distance to the closest existing center\n",
        "            rand_vals = torch.rand(n_local_trials).to(current_pot.device) * current_pot\n",
        "            candidate_ids = torch.searchsorted(torch.cumsum(closest_dist_sq.flatten(), dim=0), rand_vals)\n",
        "\n",
        "            # Numerical imprecision can result in a candidate_id out of range\n",
        "            torch.clip(candidate_ids, min=None, max=closest_dist_sq.shape[0] - 1, out=candidate_ids)\n",
        "\n",
        "            # Compute distances to center candidates\n",
        "            distance_to_candidates = torch.cdist(X[candidate_ids].unsqueeze(dim=0), X).pow(2).squeeze()\n",
        "\n",
        "            # update closest distances squared and potential for each candidate\n",
        "            torch.minimum(closest_dist_sq, distance_to_candidates, out=distance_to_candidates)\n",
        "            candidates_pot = distance_to_candidates.sum(dim=1)\n",
        "\n",
        "            # Decide which candidate is the best\n",
        "            best_candidate = torch.argmin(candidates_pot)\n",
        "            current_pot = candidates_pot[best_candidate]\n",
        "            closest_dist_sq = distance_to_candidates[best_candidate]\n",
        "            best_candidate = candidate_ids[best_candidate]\n",
        "\n",
        "            # Permanently add best center candidate found in local tries\n",
        "            centers.append(X[best_candidate])\n",
        "\n",
        "        centers = torch.stack(centers, dim=0).to(dtype=X.dtype)\n",
        "        return centers\n",
        "\n",
        "\n",
        "class GCN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Convolutional Network\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=2, dropout=0.5, batchnorm=False, activation=\"relu\"):\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        assert activation in [\"relu\", \"elu\"]\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(GCNConv(in_channels, hidden_channels, cached=True))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        if batchnorm:\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GCNConv(hidden_channels, hidden_channels, cached=True))\n",
        "            if batchnorm:\n",
        "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(GCNConv(hidden_channels, out_channels, cached=True))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.embed(x, adj_t)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            if len(self.bns) > 0:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class SAGE(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    GraphSAGE\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels,\n",
        "                 num_layers=2, dropout=0.5, batchnorm=False, activation=\"relu\"):\n",
        "        super(SAGE, self).__init__()\n",
        "\n",
        "        assert activation in [\"relu\", \"elu\"]\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(SAGEConv(in_channels, hidden_channels))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        if batchnorm:\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\n",
        "            if batchnorm:\n",
        "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(SAGEConv(hidden_channels, out_channels))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.embed(x, adj_t)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            if len(self.bns) > 0:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x\n",
        "\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Graph Attention Network\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, hidden_channels, out_channels, num_heads,\n",
        "                 num_layers=2, dropout=0.5, batchnorm=False, activation=\"relu\"):\n",
        "        super(GAT, self).__init__()\n",
        "\n",
        "        assert activation in [\"relu\", \"elu\"]\n",
        "\n",
        "        self.convs = torch.nn.ModuleList()\n",
        "        self.convs.append(\n",
        "            GATConv(in_channels, hidden_channels, heads=num_heads, bias=False))\n",
        "        self.bns = torch.nn.ModuleList()\n",
        "        if batchnorm:\n",
        "            self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "\n",
        "        self.num_layers = num_layers\n",
        "        for _ in range(num_layers - 2):\n",
        "            self.convs.append(\n",
        "                GATConv(hidden_channels * num_heads, hidden_channels, heads=num_heads, bias=False))\n",
        "            if batchnorm:\n",
        "                self.bns.append(torch.nn.BatchNorm1d(hidden_channels))\n",
        "        self.convs.append(\n",
        "            GATConv(hidden_channels * num_heads, out_channels, heads=num_heads, bias=False))\n",
        "\n",
        "        self.dropout = dropout\n",
        "        self.activation = getattr(F, activation)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        x = self.embed(x, adj_t)\n",
        "        x = self.convs[-1](x, adj_t)\n",
        "        return x\n",
        "\n",
        "    def embed(self, x, adj_t):\n",
        "        for i, conv in enumerate(self.convs[:-1]):\n",
        "            x = conv(x, adj_t)\n",
        "            if len(self.bns) > 0:\n",
        "                x = self.bns[i](x)\n",
        "            x = self.activation(x)\n",
        "            x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        return x"
      ],
      "metadata": {
        "id": "59GVrcE2vcEx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Partition"
      ],
      "metadata": {
        "id": "ML1Jj9LQJtP6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GraphPartition:\n",
        "\n",
        "    def __init__(self, graph, x, num_classes):\n",
        "\n",
        "        self.graph = graph\n",
        "        self.x = x\n",
        "        self.n_cluster = num_classes\n",
        "        self.costs = []\n",
        "\n",
        "    def clauset_newman_moore(self, num_part=-1, weight=None, q_break=0):\n",
        "        \"\"\"\n",
        "        Find communities in graph using Clauset-Newman-Moore greedy modularity maximization.\n",
        "\n",
        "        Greedy modularity maximization begins with each node in its own community\n",
        "        and joins the pair of communities that most increases (least decrease) modularity\n",
        "        until q_break.\n",
        "\n",
        "        Modified from\n",
        "        https://networkx.org/documentation/stable/_modules/networkx/algorithms/community/modularity_max.html#greedy_modularity_communities\n",
        "        \"\"\"\n",
        "\n",
        "        # Count nodes and edges\n",
        "        N = len(self.graph.nodes())\n",
        "        m = sum([d.get(\"weight\", 1) for u, v, d in self.graph.edges(data=True)])\n",
        "        q0 = 1.0 / (2.0 * m)\n",
        "\n",
        "        # Map node labels to contiguous integers\n",
        "        label_for_node = {i: v for i, v in enumerate(self.graph.nodes())}\n",
        "        node_for_label = {label_for_node[i]: i for i in range(N)}\n",
        "\n",
        "        # Calculate edge weight\n",
        "        if weight is not None:\n",
        "            edge_weight = []\n",
        "            for edge in self.graph.edges:\n",
        "                edge_weight.append(torch.linalg.norm(self.x[edge[0]] - self.x[edge[1]]).item())\n",
        "            edge_weight = torch.tensor(edge_weight)\n",
        "            edge_weight -= edge_weight.min()\n",
        "            edge_weight /= edge_weight.max()\n",
        "            attrs = {}\n",
        "            for edge, distance in zip(self.graph.edges, list(edge_weight)):\n",
        "                attrs[edge] = {'distance': distance}\n",
        "            weight = 'distance'\n",
        "\n",
        "        # Calculate degrees\n",
        "        k_for_label = self.graph.degree(self.graph.nodes(), weight=weight)\n",
        "        k = [k_for_label[label_for_node[i]] for i in range(N)]\n",
        "\n",
        "        # Initialize community and merge lists\n",
        "        communities = {i: frozenset([i]) for i in range(N)}\n",
        "\n",
        "        # Initial modularity and homophily\n",
        "        partition = [[label_for_node[x] for x in c] for c in communities.values()]\n",
        "        q_cnm = modularity(self.graph, partition)\n",
        "\n",
        "        # Initialize data structures\n",
        "        # CNM Eq 8-9 (Eq 8 was missing a factor of 2 (from A_ij + A_ji)\n",
        "        # a[i]: fraction of edges within community i\n",
        "        # dq_dict[i][j]: dQ for merging community i, j\n",
        "        # dq_heap[i][n] : (-dq, i, j) for communitiy i nth largest dQ\n",
        "        # H[n]: (-dq, i, j) for community with nth largest max_j(dQ_ij)\n",
        "        a = [k[i] * q0 for i in range(N)]\n",
        "        dq_dict = {\n",
        "            i: {\n",
        "                j: 2 * q0 - 2 * k[i] * k[j] * q0 * q0\n",
        "                for j in [node_for_label[u] for u in self.graph.neighbors(label_for_node[i])]\n",
        "                if j != i\n",
        "            }\n",
        "            for i in range(N)\n",
        "        }\n",
        "        dq_heap = [\n",
        "            MappedQueue([(-dq, i, j) for j, dq in dq_dict[i].items()]) for i in range(N)\n",
        "        ]\n",
        "        H = MappedQueue([dq_heap[i].heap[0] for i in range(N) if len(dq_heap[i]) > 0])\n",
        "\n",
        "        # Merge communities until we can't improve modularity\n",
        "        while len(H) > 1:\n",
        "            # Find best merge\n",
        "            # Remove from heap of row maxes\n",
        "            # Ties will be broken by choosing the pair with lowest min community id\n",
        "            try:\n",
        "                dq, i, j = H.pop()\n",
        "            except IndexError:\n",
        "                break\n",
        "            dq = -dq\n",
        "\n",
        "            # Remove best merge from row i heap\n",
        "            dq_heap[i].pop()\n",
        "\n",
        "            # Push new row max onto H\n",
        "            if len(dq_heap[i]) > 0:\n",
        "                H.push(dq_heap[i].heap[0])\n",
        "\n",
        "            # If this element was also at the root of row j, we need to remove the\n",
        "            # duplicate entry from H\n",
        "            if dq_heap[j].heap[0] == (-dq, j, i):\n",
        "                H.remove((-dq, j, i))\n",
        "                # Remove best merge from row j heap\n",
        "                dq_heap[j].remove((-dq, j, i))\n",
        "                # Push new row max onto H\n",
        "                if len(dq_heap[j]) > 0:\n",
        "                    H.push(dq_heap[j].heap[0])\n",
        "            else:\n",
        "                # Duplicate wasn't in H, just remove from row j heap\n",
        "                dq_heap[j].remove((-dq, j, i))\n",
        "\n",
        "            # Stop when change is non-positive 0\n",
        "            if 0 < num_part == len(communities):\n",
        "                break\n",
        "            elif dq <= q_break:\n",
        "                break\n",
        "\n",
        "            # New modularity and homophily\n",
        "            q_cnm += dq\n",
        "\n",
        "            # Perform merge\n",
        "            communities[j] = frozenset(communities[i] | communities[j])\n",
        "            del communities[i]\n",
        "\n",
        "            # Get list of communities connected to merged communities\n",
        "            i_set = set(dq_dict[i].keys())\n",
        "            j_set = set(dq_dict[j].keys())\n",
        "            all_set = (i_set | j_set) - {i, j}\n",
        "            both_set = i_set & j_set\n",
        "\n",
        "            # Merge i into j and update dQ\n",
        "            for k in all_set:\n",
        "\n",
        "                # Calculate new dq value\n",
        "                if k in both_set:\n",
        "                    dq_jk = dq_dict[j][k] + dq_dict[i][k]\n",
        "                elif k in j_set:\n",
        "                    dq_jk = dq_dict[j][k] - 2.0 * a[i] * a[k]\n",
        "                else:\n",
        "                    # k in i_set\n",
        "                    dq_jk = dq_dict[i][k] - 2.0 * a[j] * a[k]\n",
        "\n",
        "                # Update rows j and k\n",
        "                for row, col in [(j, k), (k, j)]:\n",
        "                    # Save old value for finding heap index\n",
        "                    if k in j_set:\n",
        "                        d_old = (-dq_dict[row][col], row, col)\n",
        "                    else:\n",
        "                        d_old = None\n",
        "                    # Update dict for j,k only (i is removed below)\n",
        "                    dq_dict[row][col] = dq_jk\n",
        "                    # Save old max of per-row heap\n",
        "                    if len(dq_heap[row]) > 0:\n",
        "                        d_oldmax = dq_heap[row].heap[0]\n",
        "                    else:\n",
        "                        d_oldmax = None\n",
        "                    # Add/update heaps\n",
        "                    d = (-dq_jk, row, col)\n",
        "                    if d_old is None:\n",
        "                        # We're creating a new nonzero element, add to heap\n",
        "                        dq_heap[row].push(d)\n",
        "                    else:\n",
        "                        # Update existing element in per-row heap\n",
        "                        dq_heap[row].update(d_old, d)\n",
        "                    # Update heap of row maxes if necessary\n",
        "                    if d_oldmax is None:\n",
        "                        # No entries previously in this row, push new max\n",
        "                        H.push(d)\n",
        "                    else:\n",
        "                        # We've updated an entry in this row, has the max changed?\n",
        "                        if dq_heap[row].heap[0] != d_oldmax:\n",
        "                            H.update(d_oldmax, dq_heap[row].heap[0])\n",
        "\n",
        "            # Remove row/col i from matrix\n",
        "            i_neighbors = dq_dict[i].keys()\n",
        "            for k in i_neighbors:\n",
        "                # Remove from dict\n",
        "                dq_old = dq_dict[k][i]\n",
        "                del dq_dict[k][i]\n",
        "                # Remove from heaps if we haven't already\n",
        "                if k != j:\n",
        "                    # Remove both row and column\n",
        "                    for row, col in [(k, i), (i, k)]:\n",
        "                        # Check if replaced dq is row max\n",
        "                        d_old = (-dq_old, row, col)\n",
        "                        if dq_heap[row].heap[0] == d_old:\n",
        "                            # Update per-row heap and heap of row maxes\n",
        "                            dq_heap[row].remove(d_old)\n",
        "                            H.remove(d_old)\n",
        "                            # Update row max\n",
        "                            if len(dq_heap[row]) > 0:\n",
        "                                H.push(dq_heap[row].heap[0])\n",
        "                        else:\n",
        "                            # Only update per-row heap\n",
        "                            dq_heap[row].remove(d_old)\n",
        "\n",
        "            del dq_dict[i]\n",
        "            # Mark row i as deleted, but keep placeholder\n",
        "            dq_heap[i] = MappedQueue()\n",
        "            # Merge i into j and update a\n",
        "            a[j] += a[i]\n",
        "            a[i] = 0\n",
        "\n",
        "        communities = [\n",
        "            [label_for_node[i] for i in c] for c in communities.values()\n",
        "        ]\n",
        "        return sorted(communities, key=len, reverse=True)\n",
        "\n",
        "    def agglomerative_clustering(self, communities, min_clusters=2):\n",
        "        \"\"\"\n",
        "        Agglomerative Clustering: Ward's Linkage Method\n",
        "        \"\"\"\n",
        "\n",
        "        n_clusters = list(range(min_clusters, len(communities)))\n",
        "        n_clusters.reverse()\n",
        "        partitions = {}\n",
        "\n",
        "        dist, x_com = self.community_linkage(communities, full=True)\n",
        "\n",
        "        num_clusters = len(communities)\n",
        "        while num_clusters > min(n_clusters):\n",
        "\n",
        "            sorted_communities = sorted(communities, key=lambda c: len(c), reverse=True)\n",
        "            partitions[num_clusters] = torch.zeros(self.x.shape[0], dtype=torch.int)\n",
        "            for i, com in enumerate(sorted_communities):\n",
        "                partitions[num_clusters][com] = i\n",
        "\n",
        "            merge_cost, closest_idx = torch.min(dist, dim=1)\n",
        "            j = torch.argmin(merge_cost).item()\n",
        "            i = closest_idx[j].item()\n",
        "            assert i > j\n",
        "\n",
        "            communities[j].extend(communities[i])\n",
        "            del communities[i]\n",
        "            x_com = torch.cat((x_com[0:i], x_com[i + 1:]), dim=0)\n",
        "            x_com[j] = self.x[communities[j]].mean(axis=0)\n",
        "\n",
        "            dist = torch.cat((dist[0:i], dist[i + 1:]), dim=0)\n",
        "            dist = torch.cat((dist[:, 0:i], dist[:, i + 1:]), dim=1)\n",
        "            num_clusters -= 1\n",
        "\n",
        "            for k in range(len(communities)):\n",
        "                if k == j:\n",
        "                    continue\n",
        "                nk, nj = len(communities[k]), len(communities[j])\n",
        "                n = nk * nj / (nk + nj)\n",
        "                d = torch.linalg.norm(x_com[j] - x_com[k])\n",
        "                dist[k, j] = d * n\n",
        "                dist[j, k] = d * n\n",
        "\n",
        "            cost = merge_cost.min().item()\n",
        "            self.costs.append(cost)\n",
        "\n",
        "        return partitions\n",
        "\n",
        "    def community_linkage(self, communities, full=True):\n",
        "\n",
        "        n = self.x.shape[1]\n",
        "        x_com = []\n",
        "        for com in communities:\n",
        "            x_com.append(self.x[com].mean(axis=0))\n",
        "        x_com = torch.stack(x_com, dim=0)\n",
        "\n",
        "        linkage = torch.linalg.norm(\n",
        "            x_com.reshape(1, -1, n) - x_com.reshape(-1, 1, n), dim=2\n",
        "        )\n",
        "        for i in range(len(communities)):\n",
        "            for j in range(i + 1, len(communities)):\n",
        "                ni, nj = len(communities[i]), len(communities[j])\n",
        "                n = ni * nj / (ni + nj)\n",
        "                linkage[i, j] *= n\n",
        "                linkage[j, i] *= n\n",
        "        linkage += torch.diag(torch.ones(linkage.shape[0]) * float(\"Inf\"))\n",
        "\n",
        "        if full:\n",
        "            return linkage, x_com\n",
        "        return linkage\n"
      ],
      "metadata": {
        "id": "5wAtl9IcHdZ3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KiCfeaqUuSi"
      },
      "source": [
        "## Experiment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "GZpPbxiQLpZW"
      },
      "outputs": [],
      "source": [
        "def run(data, args):\n",
        "\n",
        "    gnn = args.model\n",
        "    baseline = args.baselines\n",
        "    budget = int(args.budget)\n",
        "    seed = int(args.seed)\n",
        "\n",
        "    # Choose model\n",
        "    model_args = {\n",
        "        \"in_channels\": data.num_features,\n",
        "        \"out_channels\": data.num_classes,\n",
        "        \"hidden_channels\": args.hidden,\n",
        "        \"num_layers\": args.num_layers,\n",
        "        \"dropout\": args.dropout,\n",
        "        \"activation\": args.activation,\n",
        "        \"batchnorm\": args.batchnorm\n",
        "    }\n",
        "\n",
        "    # Initialize models\n",
        "    if gnn == \"gat\":\n",
        "        model_args[\"num_heads\"] = args.num_heads\n",
        "        model_args[\"hidden_channels\"] = int(args.hidden / args.num_heads)\n",
        "        model = GAT(**model_args)\n",
        "    elif gnn == \"gcn\":\n",
        "        model = GCN(**model_args)\n",
        "    elif gnn == \"sage\":\n",
        "        model = SAGE(**model_args)\n",
        "    else:\n",
        "        raise NotImplemented\n",
        "\n",
        "    model = model.to(args.device)\n",
        "\n",
        "    # General-Purpose Methods\n",
        "    if baseline == \"random\":\n",
        "        agent = Random(data, model, seed, args)\n",
        "    elif baseline == \"density\":\n",
        "        agent = Density(data, model, seed, args)\n",
        "    elif baseline == \"uncertainty\":\n",
        "        agent = Uncertainty(data, model, seed, args)\n",
        "    elif baseline == \"coreset\":\n",
        "        agent = CoreSetGreedy(data, model, seed, args)\n",
        "\n",
        "    # Graph-specific Methods\n",
        "    elif baseline == \"degree\":\n",
        "        agent = Degree(data, model, seed, args)\n",
        "    elif baseline == \"pagerank\":\n",
        "        agent = PageRank(data, model, seed, args)\n",
        "    elif baseline == \"age\":\n",
        "        agent = AGE(data, model, seed, args)\n",
        "    elif baseline == \"featprop\":\n",
        "        agent = ClusterBased(data, model, seed, args,\n",
        "                              representation='aggregation',\n",
        "                              encoder='gcn')\n",
        "\n",
        "    # Our Methods\n",
        "    elif baseline == \"graphpart\":\n",
        "        agent = PartitionBased(data, model, seed, args,\n",
        "                                representation='aggregation',\n",
        "                                encoder='gcn',\n",
        "                                compensation=0)\n",
        "    elif baseline == \"graphpartfar\":\n",
        "        agent = PartitionBased(data, model, seed, args,\n",
        "                                representation='aggregation',\n",
        "                                encoder='gcn',\n",
        "                                compensation=1)\n",
        "\n",
        "    # Ablation Studies\n",
        "    elif 'part' in baseline:\n",
        "        agent = PartitionBased(data, model, seed, args,\n",
        "                                representation=args.representation,\n",
        "                                compensation=0)\n",
        "    else:\n",
        "        agent = ClusterBased(data, model, seed, args,\n",
        "                              representation=args.representation)\n",
        "\n",
        "    # Initialization\n",
        "    training_mask = np.zeros(data.num_nodes, dtype=bool)\n",
        "    initial_mask = np.arange(data.num_nodes)\n",
        "    np.random.shuffle(initial_mask)\n",
        "    init = args.init\n",
        "    if baseline in ['density', 'uncertainty', 'coreset', 'age']:\n",
        "        init = budget // 3\n",
        "    training_mask[initial_mask[:init]] = True\n",
        "\n",
        "    training_mask = torch.tensor(training_mask)\n",
        "    agent.update(training_mask)\n",
        "    agent.train()\n",
        "\n",
        "    if args.verbose > 0:\n",
        "        print('Round {:03d}: Labelled: {:d}, Prediction macro-f1 score {:.4f}'\n",
        "              .format(0, init, agent.evaluate()))\n",
        "        \n",
        "    # Query\n",
        "    start = timer()\n",
        "    indices = agent.query(budget - init)\n",
        "    end = timer()\n",
        "    print('Total Query Runtime [s]:', end - start)\n",
        "\n",
        "    # Update\n",
        "    training_mask[indices] = True\n",
        "    agent.update(training_mask)\n",
        "\n",
        "    # Training\n",
        "    agent.train()\n",
        "\n",
        "    # Evaluate\n",
        "    f1, acc = agent.evaluate()\n",
        "    labelled = len(np.where(agent.data.train_mask != 0)[0])\n",
        "\n",
        "    if args.verbose > 0:\n",
        "        print('Round {:03d}: # Labelled nodes: {:d}, Prediction macro-f1 score {:.4f}'\n",
        "              .format(rd, labelled, f1))\n",
        "    else:\n",
        "        print(\"{},{},{},{},{},{}\"\n",
        "              .format(gnn, baseline, seed,\n",
        "                      labelled, f1, acc))\n",
        "\n",
        "    return f1, acc, indices, agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ab_vTH-5hG0x"
      },
      "outputs": [],
      "source": [
        "def test(data, gnns, budgets, baselines, seed=0):\n",
        "\n",
        "    # Set seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed)\n",
        "\n",
        "    # Training settings\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\n",
        "        \"--verbose\", type=int, default=0, help=\"Verbose: 0, 1 or 2\")\n",
        "    parser.add_argument(\n",
        "        \"--device\", default=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "    # General configs\n",
        "    parser.add_argument(\n",
        "        \"--baselines\", type=str, default=baselines)\n",
        "    parser.add_argument(\n",
        "        \"--model\", default=gnns)\n",
        "    parser.add_argument(\n",
        "        \"--partition\", type=str, default='greedy')\n",
        "\n",
        "    # Active Learning parameters\n",
        "    parser.add_argument(\n",
        "        \"--budget\", type=int, default=budgets,\n",
        "        help=\"Number of rounds to run the agent.\")\n",
        "    parser.add_argument(\n",
        "        \"--retrain\", type=bool, default=True)\n",
        "    parser.add_argument(\n",
        "        \"--num_centers\", type=int, default=1)\n",
        "    parser.add_argument(\n",
        "        \"--representation\", type=str, default='aggregation')\n",
        "    parser.add_argument(\n",
        "        \"--compensation\", type=float, default=1.0)\n",
        "    parser.add_argument(\n",
        "        \"--init\", type=float, default=0, help=\"Number of initially labelled nodes.\")\n",
        "    parser.add_argument(\n",
        "        \"--epochs\", type=int, default=300, help=\"Number of epochs to train.\")\n",
        "    parser.add_argument(\n",
        "        \"--steps\", type=int, default=4, help=\"Number of steps of random walk.\")\n",
        "\n",
        "    # GNN parameters\n",
        "    parser.add_argument(\n",
        "        \"--seed\", type=int, default=seed, help=\"Number of random seeds.\")\n",
        "    parser.add_argument(\n",
        "        \"--lr\", type=float, default=0.01, help=\"Initial learning rate.\")\n",
        "    parser.add_argument(\n",
        "        \"--weight_decay\", type=float, default=5e-4,\n",
        "        help=\"Weight decay (L2 loss on parameters).\")\n",
        "    parser.add_argument(\n",
        "        \"--hidden\", type=int, default=16, help=\"Number of hidden units.\")\n",
        "    parser.add_argument(\n",
        "        \"--num_layers\", type=int, default=2, help=\"Number of layers.\")\n",
        "    parser.add_argument(\n",
        "        \"--dropout\", type=float, default=0,\n",
        "        help=\"Dropout rate (1 - keep probability).\")\n",
        "    parser.add_argument(\n",
        "        \"--batchnorm\", type=bool, default=False,\n",
        "        help=\"Perform batch normalization\")\n",
        "    parser.add_argument(\n",
        "        \"--activation\", default=\"relu\")\n",
        "\n",
        "    # GAT hyper-parameters\n",
        "    parser.add_argument(\n",
        "        \"--num_heads\", type=int, default=8, help=\"Number of heads.\")\n",
        "\n",
        "    args, _ = parser.parse_known_args()\n",
        "\n",
        "    f1_all, acc_all, queried, agent = run(data, args)\n",
        "\n",
        "    agg = agent.get_node_representation('aggregation', 'gcn').cpu().numpy()\n",
        "    agg_distance = {}\n",
        "    train_idx = list(queried.numpy())\n",
        "    test_idx = list(range(data.num_nodes))\n",
        "    train_agg = agg[train_idx]\n",
        "\n",
        "    for i in test_idx:\n",
        "        distance_tmp = train_agg - agg[i]\n",
        "        agg_distance[int(i)] = float(min(np.linalg.norm(distance_tmp, axis=1)))\n",
        "    \n",
        "    group_num = 10\n",
        "    sort_res = list(\n",
        "        map(lambda x: x[0], sorted(agg_distance.items(), key=lambda x: x[1])))\n",
        "    node_num_group = len(sort_res) // group_num\n",
        "    res = [\n",
        "        sort_res[i:i + node_num_group + 1]\n",
        "        for i in range(0, len(sort_res), node_num_group + 1)\n",
        "    ]\n",
        "\n",
        "    acc_list = []\n",
        "    f1_list = []\n",
        "    agent.clf.eval()\n",
        "    logits = agent.clf(agent.data.x, agent.data.adj_t)\n",
        "    y_pred = logits.max(1)[1].cpu()\n",
        "    y_true = agent.data.y.cpu()\n",
        "    for test_set in res:\n",
        "        acc = metrics.accuracy_score(y_true[test_set], y_pred[test_set])\n",
        "        f1 = metrics.f1_score(y_true[test_set], y_pred[test_set], average='macro')\n",
        "        acc_list.append(acc)\n",
        "        f1_list.append(f1)\n",
        "\n",
        "    print(\"{},{},{},{},{},\\n{},{},{},{},{}\"\n",
        "    .format(acc_list[0], acc_list[1], acc_list[2], acc_list[3], acc_list[4],\n",
        "            acc_list[5], acc_list[6], acc_list[7], acc_list[8], acc_list[9]))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment"
      ],
      "metadata": {
        "id": "e1DyTtCy-kcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = 'cora'\n",
        "\n",
        "path = os.path.join(\"data\", name)\n",
        "dataset = Planetoid(root=path, name=name, transform=T.ToSparseTensor())\n",
        "\n",
        "data = dataset[0]\n",
        "data.max_part = 7\n",
        "data.num_classes = dataset.num_classes\n",
        "data.params = {'age': [0.05, 0.05, 0.9]}\n",
        "\n",
        "print(data.num_nodes)\n",
        "print(data.num_edges)\n",
        "print(data.num_classes)\n",
        "print(data.x.shape[1])\n",
        "\n",
        "data.printname = name\n",
        "data.adj_t = data.adj_t.to_symmetric() if not isinstance(data.adj_t, torch.Tensor) else data.adj_t\n",
        "edges = [(int(i), int(j)) for i, j in zip(data.adj_t.storage._row,\n",
        "                                          data.adj_t.storage._col)]\n",
        "\n",
        "data.g = nx.Graph()\n",
        "data.g.add_edges_from(edges)\n",
        "graph = data.g.to_undirected()\n",
        "edges = [(int(i), int(i)) for i in range(data.num_nodes)]\n",
        "data.g.add_edges_from(edges)\n",
        "\n",
        "feat_dim = data.x.size(1)\n",
        "conv = GCNConv(feat_dim, feat_dim, cached=True, bias=False)\n",
        "conv.lin.weight = torch.nn.Parameter(torch.eye(feat_dim))\n",
        "with torch.no_grad():\n",
        "    data.aggregated = conv(data.x, data.adj_t)\n",
        "    data.aggregated = conv(data.aggregated, data.adj_t)\n",
        "    \n",
        "filename = \"data/partitions.json\"\n",
        "if os.path.exists(filename):\n",
        "    data.partitions = {}\n",
        "    obj_text = codecs.open(filename, 'r', encoding='utf-8').read()\n",
        "    part_dict = json.loads(obj_text)\n",
        "    data.max_part = part_dict[name]['num_part']\n",
        "    data.partitions[data.max_part] = torch.tensor(part_dict[name]['partition'])\n",
        "else:\n",
        "    print('Partition file not found!')\n",
        "    raise NotImplemented"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkANBuBDyVX9",
        "outputId": "c3dd3dce-02ac-4396-9eae-80779876d90c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2708\n",
            "10556\n",
            "7\n",
            "1433\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test(data, 'gcn', 40, 'graphpart', seed=0)\n",
        "test(data, 'gcn', 40, 'graphpartfar', seed=0)\n",
        "test(data, 'gcn', 40, 'random', seed=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_bvivahygBk",
        "outputId": "bfe6dad6-547d-4a39-bfd5-06305ebb3d6b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Query Runtime [s]: 3.956631758999947\n",
            "gcn,graphpart,0,40,0.7985027144005709,0.8142540620384048\n",
            "0.915129151291513,0.8523985239852399,0.8228782287822878,0.8302583025830258,0.8339483394833949,\n",
            "0.8339483394833949,0.7712177121771218,0.8118081180811808,0.7749077490774908,0.6951672862453532\n",
            "Total Query Runtime [s]: 5.6262782779999725\n",
            "gcn,graphpartfar,0,40,0.7702959697159066,0.7961595273264401\n",
            "0.9261992619926199,0.8302583025830258,0.7601476014760148,0.8228782287822878,0.7859778597785978,\n",
            "0.7970479704797048,0.7970479704797048,0.7601476014760148,0.7527675276752768,0.7286245353159851\n",
            "Total Query Runtime [s]: 0.0003246920000492537\n",
            "gcn,random,0,40,0.5655700544372413,0.6063515509601182\n",
            "0.8302583025830258,0.7121771217712177,0.6715867158671587,0.6826568265682657,0.6346863468634686,\n",
            "0.4981549815498155,0.5239852398523985,0.5202952029520295,0.45387453874538747,0.5353159851301115\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "iQvaanZISHyN",
        "IRVGdGWtIMrD",
        "_71i5TaccNFH"
      ],
      "name": "main.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.4.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}